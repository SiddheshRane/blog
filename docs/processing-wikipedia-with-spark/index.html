<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <title>Processing Wikipedia with Spark</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="What Beginners Wished They Knew">
        <meta name="author" content="Siddhesh Rane">
        <!--<meta name="keywords" content="" th:if="${content.tags != null}" th:content="${#strings.listJoin(content.tags, ' , ')} ?: '' ">-->
        <meta name="generator" content="JBake">
        
        <!--Facebook Card-->
		<meta property="og:type" content="article" />
		<meta property="og:title"  content="Processing Wikipedia with Spark" />
		<meta property="og:description" content="What Beginners Wished They Knew"/>
		<meta property="og:image" content="http://localhost:8820/processing-wikipedia-with-spark/spark-bulb.jpg" />
        
        <!--Twitter card -->
        
        <meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:title" content="Processing Wikipedia with Spark">
		<meta name="twitter:creator" content="@unrealSiddhesh">
		<meta name="twitter:description" content="What Beginners Wished They Knew">
		<meta name="twitter:image" content="http://localhost:8820/processing-wikipedia-with-spark/spark-bulb.jpg">
		</th:if>
        <!-- Bootstrap Core CSS -->
        <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

        <!-- Theme CSS -->
        <link href="../css/coderay.css" rel="stylesheet">
        <link href="../css/asciidoctor.css" rel="stylesheet">
        <link href="../css/clean-blog.css" rel="stylesheet">
        <!-- Custom Fonts -->
        <link href="../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Lato:300&amp;text=Siddhesh%20Rane'
           rel='stylesheet' type='text/css'>
        <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
        <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <link rel="shortcut icon" href="../favicon.png"/>
    </head>

    <body>

        <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
            <div class="container-fluid">
                <!-- Brand and toggle get grouped for better mobile display -->
                <div class="navbar-header page-scroll">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                        <span class="sr-only">Toggle navigation</span>
                        Menu <i class="fa fa-bars"></i>
                    </button>
                    <a class="navbar-brand" href="../index.html">Siddhesh Rane</a>
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="../get-theme.html">Get Theme</a>
                        </li>
                        <li>
                            <a href="../about.html">About Me</a>
                        </li>
                        
                        
                        
                    </ul>
                </div>
                <!-- /.navbar-collapse -->
            </div>
            <!-- /.container -->
        </nav>

        <!-- Page Header -->
        <!-- Set your background image for this header on the line below. -->
        <header class="intro-header" style="background-image: url(&#39;../processing-wikipedia-with-spark/spark-bulb.jpg&#39;) ; background-color:#101010">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                        <div class="post-heading">
                            <h1>Processing Wikipedia with Spark</h1>
                            <h2 class="subheading">What Beginners Wished They Knew</h2>
                            <span class="meta">Posted by <b>Siddhesh Rane</b> on 
                                May 10, 2018
                                </br>
                                <span> Tags: 
                                    <span> 
                                        <a href="../tags/spark.html" class="" >spark</a></span><span> 
                                        <a href="../tags/semantic web.html" class="" >semantic web</a></span><span> 
                                        <a href="../tags/nlp.html" class="" >nlp</a></span> 
                                </span>
                            </span>
                        </div>
                    </div>
                </div>
            </div>
        </header>

        <!-- Post Content -->
        <article>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1"><div id="preamble"> 
 <div class="sectionbody"> 
  <div class="paragraph"> 
   <p>For my Btech project I had to process all articles in Wikipedia. The uncompressed dump of its plaintext is 14GB in size, which doesn’t sound like much until you run even a trivial word count pipeline on it. I had to do more than that, think matching 100 million labels over all the articles. It was no task for a single machine, so I used <a href="http://spark.apache.org">Spark</a> to distribute the load over a cluster of machines. In this post I’ll list everything that I wish I didn’t have to discover on my own.</p> 
  </div> 
 </div> 
</div> 
<div class="sect1"> 
 <h2 id="_spark_dos_and_don_ts">Spark: Dos and Don’ts</h2> 
 <div class="sectionbody"> 
  <div class="paragraph"> 
   <p>This guide assumes you have a general idea about Spark and its terminology but haven’t tried it out yet. I used Spark 2.3.0 with Dataframe based ML API. Entire cluster was self-provisioned on commodity hardware, not using cloud infrastructure.</p> 
  </div> 
  <div class="admonitionblock caution"> 
   <table> 
    <tbody>
     <tr> 
      <td class="icon"> <i class="fa icon-caution" title="Caution"></i> </td> 
      <td class="content"> As of this writing I am still new to Spark so the problems are fresh in my mind and I can perhaps relate to beginners better. I have written tips which are guaranteed to work from my personal experience, but do not religiously rely on my explanations. Some of them are mere inferences and rest are StackOverflow. </td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <div class="admonitionblock tip"> 
   <table> 
    <tbody>
     <tr> 
      <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
      <td class="content"> Tips are written in these boxes. They form the <strong>TL;DR</strong> version of the post. </td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <div class="sect2"> 
   <h3 id="_download_and_setup">Download and Setup</h3> 
   <div class="paragraph"> 
    <p>If you set out to download Spark, you’ll notice that there are various binaries available for the same version. Spark advertizes that it does not need hadoop so you might download the user-provided-hadoop version which is smaller in size. Don’t do that.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Download the Spark binary that comes with packaged hadoop dependencies </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>Although Spark does not use Hadoop’s MapReduce framework, it does have dependencies on other Hadoop libraries like HDFS and YARN. The without-hadoop version is when you already have hadoop libraries provided elsewhere</p> 
   </div> 
   <div class="paragraph"> 
    <p>Once you test the built in examples on <code>local</code> cluster, and ensure that everything is installed and working properly, proceed to set up your cluster. Spark gives you 3 options: Mesos, YARN and Stand-alone. The first two are resource allocators which control your slave nodes and Spark has to request them to allocate its own instances. As a beginner don’t increase your complexity by going that way.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Use the stand-alone cluster mode, not Mesos or YARN </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>The stand-alone cluster is the easiest to setup and comes with sensible defaults, like using all your cores for executors. It is part of the spark distribution itself and has <code>sbin/start-all.sh</code> script that can bring up the master as well as all your slaves listed in <code>conf/slaves</code> using ssh. Mesos/YARN are separate programs that are used when your cluster isn’t just a spark cluster. Also, they don’t come with sensible defaults: executors don’t use all cores on the slaves unless explicitly specified.</p> 
   </div> 
   <div class="paragraph"> 
    <p>You also have the option of a high availability mode using Zookeeper, which keeps a list of backup masters incase any master fails. Since you are a beginner you are likely not handling a 1000 node cluster where the risk of node failure is significant. You are more likely to set up a cluster on a managed cloud platform like Amazon’s or Google’s, which already take care of node failures.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> You don’t need high availability with cloud infrastructure or a small cluster </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>I had my cluster set up in a hostile environment where human factors were responsible for power failures, and nodes going off the grid. (Basically my college computer lab where diligent students turn off the machine and careless students pull out LAN cables). I could still pull off without high availability by careful choice of the master node. You wouldn’t have to worry about that.</p> 
   </div> 
   <div class="paragraph"> 
    <p>One very important aspect is the Java version you use to run Spark. Normally a higher version of Java works with something compiled for older releases but with Project Jigsaw, modularity introduced stricter isolation and boundaries in Java 9 which breaks certain things that use reflection. On Spark 2.3.0 running on Java 9, I got illegal reflection access. Java 8 had no issues.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Check the Java version you use to run Spark. Spark 2.3.0 works with Java 8 but gives illegal access errors on Java 9 </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>This will definitely change in the near future but keep that in mind till then.</p> 
   </div> 
   <div class="paragraph"> 
    <p>The stand-alone cluster is very sensitive about URLs used to resolve master and slave nodes. Suppose you start the master node like below</p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="shell">siddhesh@x360:/opt/spark-2.3.0-bin-hadoop2.7$ sbin/start-master.sh</code></pre> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>and your master is up at <code>localhost:8080</code></p> 
   </div> 
   <div class="imageblock"> 
    <div class="content"> 
     <img src="http://localhost:8820/processing-wikipedia-with-spark/spark-master-x360.png" alt="Spark master web ui at localhost:8080" width="100%"> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>By default, your PC’s hostname is chosen as the master URL address. <code>x360</code> resolves to <code>localhost</code> but starting a slave like below will <strong>not work</strong></p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="shell"># does not work
siddhesh@x360:~/opt/spark-2.3.0-bin-hadoop2.7$ sbin/start-slave.sh spark://localhost:7077</code></pre> 
    </div> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Specify the master URL exactly as is. Do not resolve domain names to IP adresses or vice-versa </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="shell"># works
siddhesh@x360:~/opt/spark-2.3.0-bin-hadoop2.7$ sbin/start-slave.sh spark://x360:7077</code></pre> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>This works and our slave has been added to the cluster</p> 
   </div> 
   <div class="imageblock"> 
    <div class="content"> 
     <img src="http://localhost:8820/processing-wikipedia-with-spark/spark-slave-172.png" alt="slave with ip address in 172.x.x.x domain" width="100%"> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>Our slave has IP address in the 172.17.x.x subdomain which is actually the subdomain set up by docker on my machine. The master can communicate with this slave because both are on the same machine, but the slave cannot communicate with other slaves on the network or a master on a different machine, because its IP address is not routable. Like in the master case above, a slave on a machine without master will take up the hostname of the machine. When you have identical machines all of them end up using the same hostname as their address. This creates a total mess and no one can communicate with the other.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Explicitly control the IP addresses taken by master and slaves using <code>-h</code> option in start scripts or <code>SPARK_LOCAL_IP</code> environment variable. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>So the above commands would change to</p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="shell"># start master
siddhesh@master:~/opt/spark-2.3.0-bin-hadoop2.7$ sbin/start-master.sh -h $myIP
# start slave
siddhesh@slave:~/opt/spark-2.3.0-bin-hadoop2.7$ sbin/start-slave.sh -h $myIP spark://&lt;masterIP&gt;:7077
# submit a job
siddhesh@driver:~/opt/spark-2.3.0-bin-hadoop2.7$ SPARK_LOCAL_IP=$myIP bin/spark-submit ...</code></pre> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>where <code>myIP</code> is the IP address of the machine which is routable between the cluster nodes. It is more likely that all nodes are on the same network so you can write a script which will set <code>myIP</code> on each machine.</p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="shell"># assume all nodes in the 10.1.26.x subdomain
siddhesh@master:~$ myIP=`hostname -I | tr " " "\n" | grep 10.1.26. | head`</code></pre> 
    </div> 
   </div> 
  </div> 
  <div class="sect2"> 
   <h3 id="_flow_of_the_code">Flow of the Code</h3> 
   <div class="paragraph"> 
    <p>So far we have set up our cluster and see that it is functional. Now its time to code. Spark is quite well documented and comes with lots of examples so its very easy to get started with coding. What is less obvious is how the whole thing works which results in some very hard to debug errors during runtime. Suppose you coded something like this</p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="java"><span class="type">class</span> <span class="class">SomeClass</span> {
<span class="directive">static</span> SparkSession spark;
<span class="directive">static</span> LongAccumulator numSentences;

    <span class="directive">public</span> <span class="directive">static</span> <span class="type">void</span> main(<span class="predefined-type">String</span><span class="type">[]</span> args) {
             spark = SparkSession.builder()
                     .appName(<span class="string"><span class="delimiter">"</span><span class="content">Sparkl</span><span class="delimiter">"</span></span>)
                     .getOrCreate(); <i class="conum" data-value="1"></i><b>(1)</b>
      numSentences = spark.sparkContext().longAccumulator(<span class="string"><span class="delimiter">"</span><span class="content">sentences</span><span class="delimiter">"</span></span>); <i class="conum" data-value="2"></i><b>(2)</b>
      spark.read().textFile(args[<span class="integer">0</span>]).foreach(SomeClass::countSentences) <i class="conum" data-value="3"></i><b>(3)</b>
    }
    <span class="directive">static</span> <span class="type">void</span> countSentences(<span class="predefined-type">String</span> s) { numSentences.add(<span class="integer">1</span>); } <i class="conum" data-value="4"></i><b>(4)</b>
}</code></pre> 
    </div> 
   </div> 
   <div class="colist arabic"> 
    <table> 
     <tbody>
      <tr> 
       <td><i class="conum" data-value="1"></i><b>1</b></td> 
       <td>create a spark session</td> 
      </tr> 
      <tr> 
       <td><i class="conum" data-value="2"></i><b>2</b></td> 
       <td>create a long counter to keep track of job progress</td> 
      </tr> 
      <tr> 
       <td><i class="conum" data-value="3"></i><b>3</b></td> 
       <td>traverse a file line by line calling countSentences for each line</td> 
      </tr> 
      <tr> 
       <td><i class="conum" data-value="4"></i><b>4</b></td> 
       <td>add 1 to the accumulator for each sentence</td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>The above code works on a <code>local</code> cluster but will fail with a null pointer exception when run on a multinode cluster. Both <code>spark</code> as well as <code>numSentences</code> will be null on the slave machine. To solve this problem</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Encapsulate all initialized state in non-static fields of an object. Use <code>main</code> to create the object and defer further processing to it. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>What you need to understand is that the code you write is run by the driver node exactly as is, but what the slave nodes execute is a serialized job that spark gives them. Your classes will be loaded by the JVM on the slave and static initializers will run as expected but functions like <code>main</code> won’t so static values initialized in the driver won’t be seen in the slave. I am not sure how the whole thing works and am only inferring from experience so take my explanation with a grain of salt. So your code now looks like</p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="java"><span class="type">class</span> <span class="class">SomeClass</span> {
SparkSession spark; <i class="conum" data-value="1"></i><b>(1)</b>
LongAccumulator numSentences;
<span class="predefined-type">String</span><span class="type">[]</span> args;

    SomeClass(<span class="predefined-type">String</span><span class="type">[]</span> args) { <span class="local-variable">this</span>.args = args; }

    <span class="directive">public</span> <span class="directive">static</span> <span class="type">void</span> main(<span class="predefined-type">String</span><span class="type">[]</span> args){
        <span class="keyword">new</span> SomeClass(args).process(); <i class="conum" data-value="2"></i><b>(2)</b>
    }

    <span class="type">void</span> process() {
        spark = SparkSession.builder()
                .appName(<span class="string"><span class="delimiter">"</span><span class="content">Sparkl</span><span class="delimiter">"</span></span>)
                .getOrCreate();
      numSentences = spark.sparkContext().longAccumulator(<span class="string"><span class="delimiter">"</span><span class="content">sentences</span><span class="delimiter">"</span></span>);
      spark.read().textFile(args[<span class="integer">0</span>]).foreach(<span class="local-variable">this</span>::countSentences); <i class="conum" data-value="3"></i><b>(3)</b>
    }
    <span class="type">void</span> countSentences(<span class="predefined-type">String</span> s) { numSentences.add(<span class="integer">1</span>); }
}</code></pre> 
    </div> 
   </div> 
   <div class="colist arabic"> 
    <table> 
     <tbody>
      <tr> 
       <td><i class="conum" data-value="1"></i><b>1</b></td> 
       <td>Make fields non static</td> 
      </tr> 
      <tr> 
       <td><i class="conum" data-value="2"></i><b>2</b></td> 
       <td>create instance of the class and then execute spark jobs</td> 
      </tr> 
      <tr> 
       <td><i class="conum" data-value="3"></i><b>3</b></td> 
       <td>reference to <code>this</code> in the foreach lambda brings the object in the closure of accessible objects and thus gets serialized and sent to all slaves</td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>Those of you who are programming in Scala might use Scala <code>object</code>s which are singleton classes and hence may never come across this problem. Nevertheless, it is something you should know.</p> 
   </div> 
  </div> 
  <div class="sect2"> 
   <h3 id="_submit_app_and_dependencies">Submit App and Dependencies</h3> 
   <div class="paragraph"> 
    <p>There is more to coding above but before that you need to submit your application to the cluster. Unless your app is extremely trivial, chances are you are using external libraries. When you submit your app jar you also need to tell Spark the dependent libraries that you are using, so it will make them available on all nodes. It is pretty straightforward. The syntax is</p> 
   </div> 
   <div class="literalblock"> 
    <div class="content"> 
     <pre>bin/spark-submit --packages org.apache.opennlp:opennlp-tools:1.8.4,groupId:artifactId:version, ...</pre> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>I have had no issues with this scheme. It works flawlessly. I generally develop on my laptop and then submit jobs from a node on the cluster. So I need to transfer the app and its dependencies to whatever node I ssh into. Spark looks for dependencies in the local maven repo, then the central repo and any repos you specify using <code>--repositories</code> option. It is a little cumbersome to sync all that on the driver and then type out all those dependencies on the command line. So I prefer all dependencies packaged in a single jar, called an uber jar.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Use Maven shade plugin to generate an uber jar with all dependencies so job submitting becomes easier </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>Just include the following lines in your <code>pom.xml</code></p> 
   </div> 
   <div class="listingblock"> 
    <div class="content"> 
     <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;build&gt;</span>
  <span class="tag">&lt;plugins&gt;</span>
    <span class="tag">&lt;plugin&gt;</span>
      <span class="tag">&lt;groupId&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/groupId&gt;</span>
      <span class="tag">&lt;artifactId&gt;</span>maven-shade-plugin<span class="tag">&lt;/artifactId&gt;</span>
      <span class="tag">&lt;version&gt;</span>3.0.0<span class="tag">&lt;/version&gt;</span>
      <span class="tag">&lt;configuration&gt;</span>
        <span class="tag">&lt;artifactSet&gt;</span>
          <span class="tag">&lt;excludes&gt;</span>
            <span class="tag">&lt;exclude&gt;</span>org.apache.spark:*<span class="tag">&lt;/exclude&gt;</span>
          <span class="tag">&lt;/excludes&gt;</span>
        <span class="tag">&lt;/artifactSet&gt;</span>
      <span class="tag">&lt;/configuration&gt;</span>
      <span class="tag">&lt;executions&gt;</span>
        <span class="tag">&lt;execution&gt;</span>
          <span class="tag">&lt;phase&gt;</span>package<span class="tag">&lt;/phase&gt;</span>
          <span class="tag">&lt;goals&gt;</span>
            <span class="tag">&lt;goal&gt;</span>shade<span class="tag">&lt;/goal&gt;</span>
          <span class="tag">&lt;/goals&gt;</span>
        <span class="tag">&lt;/execution&gt;</span>
      <span class="tag">&lt;/executions&gt;</span>
    <span class="tag">&lt;/plugin&gt;</span>
  <span class="tag">&lt;/plugins&gt;</span>
<span class="tag">&lt;/build&gt;</span></code></pre> 
    </div> 
   </div> 
   <div class="paragraph"> 
    <p>When you build and package your project, the default distribution jar will have all dependencies included.</p> 
   </div> 
   <div class="paragraph"> 
    <p>As you submit jobs the application jars get accumulated in the <code>work</code> directory and fill up over time.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Set <code>spark.worker.cleanup.enabled</code> to true in <code>conf/spark-defaults.conf</code> </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>This option is false by default and is applicable to the stand-alone mode.</p> 
   </div> 
  </div> 
  <div class="sect2"> 
   <h3 id="_input_and_output_files">Input and Output files</h3> 
   <div class="paragraph"> 
    <p>This was the most confusing part that was difficult to diagnose.</p> 
   </div> 
   <div class="paragraph"> 
    <p>Spark supports reading/writing of various sources such as <code>hdfs</code>, <code>ftp</code>, <code>jdbc</code> or local files on the system when the protocol is <code>file://</code> or missing. My first attempt was to read from a file on my driver. I assumed that the driver would read the file, turn it into partitions and then distribute those across the cluster. Turns out it doesn’t work that way.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> When you <code>read</code> a file from the local filesystem, ensure that the file is present on all the worker nodes at exactly the same location. Spark does not implicitly distribute files from the driver to the workers. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>So I had to copy the file to every worker at the same location. The location of the file was passed as an argument to my app. Since the file was located in the parent folder I specified its path as <code>../wikiArticles.txt</code>. This did not work on the worker nodes.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Always pass absolute file paths for reading </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>It could be a mistake from my side but I know that the file path made it as is into the <code>textFile</code> function and it caused file not found errors.</p> 
   </div> 
   <div class="paragraph"> 
    <p>Spark supports common compression schemes so most gzipped or bzipped text files will be uncompressed before use. It might seem that compressed files will be more efficient but do not fall for that trap.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Don’t read from compressed text files, especially <code>gzip</code>. Uncompressed files are faster to process </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>Gzip cannot be uncompressed in parallel like bzip2, so nodes spend the bulk of their time uncompressing large files.</p> 
   </div> 
   <div class="paragraph"> 
    <p>It is a hassle to make the input files available on all workers. You can instead use Spark’s file broadcast mechanism. When submitting a job specify a comma separated list of input files with the <code>--files</code> option. Accessing these files requires <code>SparkFiles.get(filename)</code>. I could not find enough documentation on this feature.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> To read a file broadcasted with the <code>--files</code> option use <code>SparkFiles.get(&lt;onlyFileNameNotFullPath&gt;)</code> as the pathname in read functions. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>So a file submitted as <code>--files /opt/data/wikiAbstracts.txt</code> would be accesed as <code>SparkFiles.get("WikiAbstracts.txt")</code>. This returns a string which you can use in any read function that expects a path. Again, remember to specify absolute paths.</p> 
   </div> 
   <div class="paragraph"> 
    <p>Since my input file was 5GB gzipped, and my network was quite slow at 12MB/s I tried to use Spark’s file broadcast feature but the decompression itself was taking so long that I manually copied the file to every worker. If your network is fast enough you can use uncompressed files. Or alternatively use HDFS or FTP server.</p> 
   </div> 
   <div class="paragraph"> 
    <p>Writing files also follows the semantics of reading. I was saving my DataFrame to a csv file on the local system. Again I had the assumption that the results will be sent back to the driver node. Didn’t work for me.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> When a DataFrame is saved to local file path each worker saves its computed partitions to its own disk. No data is sent back to the driver </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>I was only getting a fraction of the results I was expecting. Initially I had misdiagnosed this problem as an error in my code. Later I found out that each worker was storing its computed results on its own disk.</p> 
   </div> 
  </div> 
  <div class="sect2"> 
   <h3 id="_partitions">Partitions</h3> 
   <div class="paragraph"> 
    <p>The number of partitions you make affects the performance. By default Spark will make as many partitions as there are cores in the cluster. This is not always optimal.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Keep an eye on how many workers are actively processing tasks. If too few, increase the number of partitions. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>If you read from a gzipped file Spark creates just one partition which will be processed by only one worker. That is also one reason why gzipped files are slow to process. I have observed slower performance with small number of large partitions as compared to large number of small partitions.</p> 
   </div> 
   <div class="admonitionblock tip"> 
    <table> 
     <tbody>
      <tr> 
       <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
       <td class="content"> Its better to explicitly set the number of partitions while reading data. </td> 
      </tr> 
     </tbody>
    </table> 
   </div> 
   <div class="paragraph"> 
    <p>You may not have to do this when reading from HDFS as Hadoop files are already partitioned.</p> 
   </div> 
  </div> 
 </div> 
</div> 
<div class="sect1"> 
 <h2 id="_wikipedia_and_dbpedia">Wikipedia and DBpedia</h2> 
 <div class="sectionbody"> 
  <div class="paragraph"> 
   <p>There is not much to write in this section as it was pretty straight forward. It is meant to make you aware of alternatives. The entire Wikipedia xml dump is 14GB compressed and 65 GB uncompressed. Most of the times you only want the plain text of the article but the dump is in MediaWiki markup so it needs some preprocessing. There are many tools available for this in various languages. Although I haven’t used them personally I am pretty sure it must be a time consuming task. But there are alternatives</p> 
  </div> 
  <div class="admonitionblock tip"> 
   <table> 
    <tbody>
     <tr> 
      <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
      <td class="content"> If all you want is the Wikipedia article plaintext, mostly for NLP, then download the dataset made available by DBpedia. </td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <div class="paragraph"> 
   <p>I used the full article dump (<code>NIF Context</code>) available at <a href="http://wiki.dbpedia.org/downloads-2016-10" class="bare">http://wiki.dbpedia.org/downloads-2016-10</a> (direct download from <a href="http://downloads.dbpedia.org/2016-10/core-i18n/en/nif_context_en.ttl.bz2">here</a>). This dataset gets rid off unwanted stuff like tables, infoboxes and references. The compressed download is 4.3GB in the <code>turtle</code> format. You can covert it to tsv like so</p> 
  </div> 
  <div class="literalblock"> 
   <div class="content"> 
    <pre># bzcat nif-context.ttl.bz2 | grep 'nif-core#isString' | sed 's/?dbpv=2016-10&amp;nif=context//' | awk '{$1=$1"\t"; $2=""; print }' &gt; url-article.tsv</pre> 
   </div> 
  </div> 
  <div class="paragraph"> 
   <p>Similar datasets are available for other properties like page links, anchor texts and so. Do check out <a href="https://dbpedia.org">DBpedia</a>.</p> 
  </div> 
 </div> 
</div> 
<div class="sect1"> 
 <h2 id="_a_word_about_databases">A word about Databases</h2> 
 <div class="sectionbody"> 
  <div class="paragraph"> 
   <p>I never quite understood why there is a plethora of databases, all so similar, and on top of that people buy database licenses. Until this project I hadn’t seriously used any. I ever only used MySQL and Apache Derby. For my project I used a SPARQL triple store database, Apache Jena TDB accessed over a REST API served by Jena Fuseki. This database would give me RDF urls, labels and predicates for all the resources mentioned in the supplied article. Every node would make a database call and only then would proceed with further processing.</p> 
  </div> 
  <div class="paragraph"> 
   <p>My workload had become IO bound as I could see near 0% CPU utilization on worker nodes. Each partition of the data would result in two SPARQL queries. In the worst case scenario, one of the two queries was taking 500-1000 seconds to process. Thankfully, the TDB database relies on linux’s memory mapping. I could map the whole DB into RAM and significantly improve performance.</p> 
  </div> 
  <div class="admonitionblock tip"> 
   <table> 
    <tbody>
     <tr> 
      <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> 
      <td class="content"> If you are IO bound and your database can fit into RAM, better run it in memory. </td> 
     </tr> 
    </tbody>
   </table> 
  </div> 
  <div class="paragraph"> 
   <p>I found a tool called <a href="https://hoytech.com/vmtouch/">vmtouch</a> which would show what percentage of the database directory had been mapped into memory. This tool also allows you to explicitly map any files/directories into the RAM and optionally lock it so it wont get paged out. My 16GB database could easily fit into my 32 GB RAM server. This boosted query performance by orders of magnitude to 1-2 seconds per query. Using a rudimentary form of database load balancing based on partition number, I could cut down my execution time to half by using 2 SPARQL servers instead of one.</p> 
  </div> 
 </div> 
</div> 
<div class="sect1"> 
 <h2 id="_conclusion">Conclusion</h2> 
 <div class="sectionbody"> 
  <div class="paragraph"> 
   <p>I truly enjoyed distributed computing on Spark. Without it I could not have completed my project. It was quite easy to take my existing app and have it run on Spark. I definitely would recommend anyone to give it a try.</p> 
  </div> 
 </div> 
</div></div>
                </div>
            </div>
        </article>
        <hr/>
        <div class="container"><div class="row"><div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    </div></div></div>
        <!-- Footer -->
        <footer>
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                        <ul class="list-inline text-center"> 
                            <li>
                                <a href="../feed.xml" title="Subscribe to RSS feed">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://twitter.com/unrealSiddhesh">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://github.com/SiddheshRane">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                            <li>
                                <a href="https://linkedin.com/in/siddhesh-rane/">
                                    <span class="fa-stack fa-lg">
                                        <i class="fa fa-circle fa-stack-2x"></i>
                                        <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                                    </span>
                                </a>
                            </li>
                        </ul>
                        <p class="copyright text-muted">&copy;  <span>Siddhesh Rane</span> <span>2018</span> | Clean Blog theme by <a href="https://startbootstrap.com/template-overviews/clean-blog/">Start Bootstrap</a> | Baked with <a href="http://jbake.org">JBake <span>v2.6.1</span></a></p>
                    </div>
                </div>
            </div>
        </footer>

        <!-- jQuery -->
        <script src="../vendor/jquery/jquery.min.js"></script>

        <!-- Bootstrap Core JavaScript -->
        <script src="../vendor/bootstrap/js/bootstrap.min.js" ></script>

        <!-- Theme JavaScript -->
        <script src="../js/clean-blog.min.js"></script>

        

    </body>

</html>
